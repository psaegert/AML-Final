{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import INN\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss, brier_score_loss, accuracy_score, confusion_matrix\n",
    "\n",
    "import GPy\n",
    "import optunity as opt\n",
    "import sobol as sb\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "with open('../data/data_train.pt', 'rb') as file:\n",
    "    X_train, y_train = pickle.load(file)\n",
    "\n",
    "X_train = X_train\n",
    "y_train = y_train\n",
    "\n",
    "print(f'{X_train.shape = }')\n",
    "print(f'{y_train.shape = }')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train.shape = (2521156, 28)\n",
      "y_train.shape = (2521156, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameter Optimization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "INN_parameters = {\n",
    "    'in_features': X_train.shape[1],\n",
    "    'out_features': y_train.shape[1],\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "loss_weights = {\n",
    "    'bce_factor': 10,\n",
    "    'dvg_factor': 1,\n",
    "    'logdet_factor': 1,\n",
    "    'rcst_factor': 1\n",
    "}\n",
    "\n",
    "lr = 5e-4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "n_epochs = 6\n",
    "batch_size = 1024"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "hyperparameter_search_space_boundaries = {\n",
    "    'n_blocks': [1, 12],\n",
    "    'n_coupling_network_hidden_layers': [1, 5],\n",
    "    'n_coupling_network_hidden_nodes': [4, 512 + 256],\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def scale_hyperparameters(hyperparameters):\n",
    "    return np.array([h * (boundaries[1] - boundaries[0]) + boundaries[0] for h, boundaries in zip(hyperparameters, hyperparameter_search_space_boundaries.values())])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross-Validation: Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        total_params += param\n",
    "    return total_params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_mean_CV_Score(score_function, hyperparameters, progress_bar_kwargs=None):\n",
    "    n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes = hyperparameters\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=20210927)\n",
    "\n",
    "    log_loss_list = np.empty(5, dtype=np.float64)\n",
    "\n",
    "    for split_index, (fit_index, val_index) in enumerate(kf.split(X_train)):\n",
    "        # create splits\n",
    "        X_fit, X_val = X_train[fit_index], X_train[val_index]\n",
    "        y_fit, y_val = torch.Tensor(y_train[fit_index]).to(device), y_train[val_index]\n",
    "\n",
    "        # scale features\n",
    "        sc_X_fit = StandardScaler()\n",
    "        X_fit_scaled = torch.Tensor(sc_X_fit.fit_transform(X_fit)).to(device)\n",
    "        X_val_scaled = torch.Tensor(sc_X_fit.transform(X_val)).to(device)\n",
    "\n",
    "        # create classifier\n",
    "        inn = INN.INN(**INN_parameters, n_blocks=n_blocks, coupling_network_layers=[n_coupling_network_hidden_nodes] * n_coupling_network_hidden_layers).to(device)\n",
    "\n",
    "        inn.train()\n",
    "\n",
    "        # fit\n",
    "        inn.fit(\n",
    "            X_fit_scaled,\n",
    "            y_fit,\n",
    "            n_epochs=n_epochs,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=Adam(inn.parameters(), lr=lr), \n",
    "            loss_weights=loss_weights,\n",
    "            verbose=1,\n",
    "            progress_bar_kwargs=progress_bar_kwargs\n",
    "        )\n",
    "\n",
    "        inn.eval()\n",
    "\n",
    "        del X_fit_scaled, y_fit\n",
    "\n",
    "        # evaluate\n",
    "        n_batches = len(X_val) // batch_size\n",
    "        y_proba_pred = np.empty((0, 2))\n",
    "        for i_batch in range(n_batches + 1):\n",
    "            y_proba_pred_new = inn.forward(X_val_scaled[i_batch * batch_size: (i_batch+1) * batch_size])[0].detach().cpu().numpy()\n",
    "            y_proba_pred = np.concatenate([y_proba_pred, y_proba_pred_new], axis=0)\n",
    "\n",
    "        log_loss_list[split_index] = score_function(y_val, y_proba_pred)\n",
    "\n",
    "        del inn, X_val_scaled\n",
    "\n",
    "    return np.mean(log_loss_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def expected_improvement(n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes, gp):\n",
    "    # compute E(q) and Var(q)\n",
    "    E_pred, Var_pred = gp.predict_noiseless(np.array([[n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes]]))\n",
    "\n",
    "    # compute gamma with the STD(q)\n",
    "    γ = (E_best - E_pred) / np.sqrt(Var_pred)\n",
    "\n",
    "    # return Expected Improvement\n",
    "    return (np.sqrt(Var_pred) * (γ * stats.norm.cdf(γ) + stats.norm.pdf(γ)))[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def initialize_GP(n_samples, progress=0):\n",
    "    Q_init = np.empty((n_samples, len(hyperparameter_search_space_boundaries)))\n",
    "    E_init = np.empty((n_samples, 1))\n",
    "\n",
    "    # initialize with sobol sequence between 0 and 1\n",
    "    for i in range(n_samples):\n",
    "        for j, boundaries in enumerate(hyperparameter_search_space_boundaries.values()):\n",
    "            Q_init[i, j] = sb.i4_sobol(len(hyperparameter_search_space_boundaries), i)[0][j]# * (boundaries[1] - boundaries[0]) + boundaries[0]\n",
    "\n",
    "    # compute scores for the initial hyperparameters\n",
    "    for i, hyperparameters in enumerate(Q_init):\n",
    "\n",
    "        # skip the ones that have already been computed\n",
    "        if progress > i:\n",
    "            continue\n",
    "\n",
    "        # scale hyperparameters according to their bounds and convert them to integers\n",
    "        hyperparameters_scaled = scale_hyperparameters(hyperparameters).round().astype(int)\n",
    "\n",
    "        # print the status\n",
    "        hyperparameters_dict = {key: hyperparameters_scaled[i] for i, key in enumerate(hyperparameter_search_space_boundaries.keys())}\n",
    "        print(f'{i+1}/{len(Q_init)}: {hyperparameters_dict}')\n",
    "        time.sleep(0.35)\n",
    "        \n",
    "        # compute cv score\n",
    "        E_init[i, :] = get_mean_CV_Score(log_loss, hyperparameters_scaled)\n",
    "        progress += 1\n",
    "\n",
    "        # save checkpoint\n",
    "        print('Storing Checkpoint...')\n",
    "        with open(f'../hyperparameter_results/INN.pt', 'wb') as file:\n",
    "            pickle.dump((Q_init, E_init), file)\n",
    "        with open(f'../hyperparameter_results/INN_progress.pt', 'wb') as file:\n",
    "            pickle.dump(progress, file)\n",
    "        print('Stored Checkpoint...')\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    return Q_init, E_init"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Optimization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "initial_n_samples = 8#16\n",
    "additional_n_samples = 24#64"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GP-Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "if train:\n",
    "    # load checkpoint if possible\n",
    "    if os.path.isfile('../hyperparameter_results/INN.pt') and os.path.isfile('../hyperparameter_results/INN_progress.pt'):\n",
    "        print('Loading Checkpoint...')\n",
    "        with open('../hyperparameter_results/INN.pt', 'rb') as file:\n",
    "            Q, E = pickle.load(file)\n",
    "        with open('../hyperparameter_results/INN_progress.pt', 'rb') as file:\n",
    "            progress = pickle.load(file)\n",
    "        print('Loaded Checkpoint')\n",
    "    else:\n",
    "        progress = 0\n",
    "    \n",
    "    # if not all initial hyperparameters have been tested, continue testing them\n",
    "    if progress < initial_n_samples:\n",
    "        print(f\"Initializing GP...\")\n",
    "        time.sleep(0.3)\n",
    "        Q, E = initialize_GP(initial_n_samples, progress=progress)\n",
    "        progress = initial_n_samples\n",
    "\n",
    "    # main GP training loop\n",
    "    print('Training GP...')\n",
    "    for k in range(progress - initial_n_samples, additional_n_samples):\n",
    "        # train Gaussian Process\n",
    "        GP = GPy.models.GPRegression(Q, E, kernel=GPy.kern.Matern52(3))\n",
    "        GP.optimize(messages=False)\n",
    "\n",
    "        # determine E_best (minimum value of E)\n",
    "        E_best = np.min(E)\n",
    "\n",
    "        # determine q_new (q with maximum expected improvement)\n",
    "        optimizer_output = opt.maximize(\n",
    "            lambda **kwargs: expected_improvement(gp=GP, **kwargs),\n",
    "            **{k: [0, 1] for k in hyperparameter_search_space_boundaries.keys()}\n",
    "        )[0]\n",
    "\n",
    "        # extract and scale new 'optimal' hyperparameters\n",
    "        q_new = np.array([optimizer_output[k] for k in hyperparameter_search_space_boundaries.keys()]).ravel()\n",
    "        q_new_scaled = scale_hyperparameters(q_new).round().astype(int)\n",
    "\n",
    "        # only for integer values: if the new hyperparameters have already been tested, the algorithm converged\n",
    "        for q in Q:\n",
    "            if (q_new == q).all():\n",
    "                print('GP Converged early.')\n",
    "                break\n",
    "\n",
    "        # print status\n",
    "        hyperparameters_dict = {key: q_new_scaled[i] for i, key in enumerate(hyperparameter_search_space_boundaries.keys())}\n",
    "        print(f'{k+1}/{additional_n_samples}: {hyperparameters_dict}')\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        # add q_new to the training set Q\n",
    "        Q = np.vstack((Q, q_new))\n",
    "\n",
    "        # add value to E\n",
    "        E = np.vstack((E, get_mean_CV_Score(log_loss, q_new_scaled).reshape(-1, 1)))\n",
    "\n",
    "        # save checkpoint\n",
    "        progress += 1\n",
    "        print('Storing Checkpoint...')\n",
    "        with open(f'../hyperparameter_results/INN.pt', 'wb') as file:\n",
    "            pickle.dump((Q, E), file)\n",
    "        with open(f'../hyperparameter_results/INN_progress.pt', 'wb') as file:\n",
    "            pickle.dump(progress, file)\n",
    "        print('Stored Checkpoint...')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print('Completed Training')\n",
    "\n",
    "else:\n",
    "    print(f'Loading Results...')\n",
    "    with open(f'../hyperparameter_results/INN.pt', 'rb') as file:\n",
    "        Q, E = pickle.load(file)\n",
    "    print(f'Loaded Results')\n",
    "\n",
    "GP = GPy.models.GPRegression(Q, E, kernel=GPy.kern.Matern52(3))\n",
    "GP.optimize(messages=False);"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2/8: {'n_blocks': 6, 'n_coupling_network_hidden_layers': 3, 'n_coupling_network_hidden_nodes': 386}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      " 67%|██████▋   | 4/6 [05:31<02:45, 82.92s/it, batch=1196/1969, weighted_loss=-73.838, bce=+0.194, dvg=+8.894, rcst=+0.403, logdet=-85.074]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-61c4ab8a6902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Initializing GP...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_GP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_n_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_n_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-712e2ea6e770>\u001b[0m in \u001b[0;36minitialize_GP\u001b[0;34m(n_samples, progress)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# compute cv score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mE_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mean_CV_Score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-08bceaf24bd0>\u001b[0m in \u001b[0;36mget_mean_CV_Score\u001b[0;34m(score_function, hyperparameters, progress_bar_kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         inn.fit(\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mX_fit_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0my_fit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UNI/Semester_6/AML/AML-Final/models/INN.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_epochs, batch_size, optimizer, loss_weights, verbose, progress_bar_kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    214\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GP-Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Q_scaled = np.round([scale_hyperparameters(q) for q in Q]).astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(30, 6))\n",
    "\n",
    "for i in range(3):\n",
    "    axes[i].scatter(Q_scaled[:5, i], E[:5])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "fig.tight_layout()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel(r'n_blocks'); ax.set_ylabel(r'n_layers'); ax.set_zlabel(r'n_nodes')\n",
    "\n",
    "x = Q_scaled[:, 0]\n",
    "y = Q_scaled[:, 1]\n",
    "z = Q_scaled[:, 2]\n",
    "c = E[:, 0]\n",
    "\n",
    "img = ax.scatter(x, y, z, c=c, cmap=plt.get_cmap('Spectral'))\n",
    "fig.subplots_adjust(right=0.7)\n",
    "colorbar_ax = fig.add_axes([0.8, 0.2, 0.03, 0.6])\n",
    "fig.colorbar(img, cax=colorbar_ax, label='\\nBCE')\n",
    "ax.view_init(elev=20., azim=220)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find 'Best' Hyperparameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def GP_log_loss_upper_confidence_bound(n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes, gp):\n",
    "    mean, var = gp.predict_noiseless(np.array([[n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes]]))\n",
    "    return mean + np.sqrt(var)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hyperparameter_best_upper_confidence_bound = opt.minimize(\n",
    "    lambda **kwargs: GP_log_loss_upper_confidence_bound(gp=GP, **kwargs),\n",
    "    **{k: [0, 1] for k in hyperparameter_search_space_boundaries.keys()}\n",
    ")[0]\n",
    "\n",
    "hyperparameter_best_upper_confidence_bound_scaled = scale_hyperparameters(hyperparameter_best_upper_confidence_bound.values()).round().astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hyperparameter_best_upper_confidence_bound_scaled"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# scale features\n",
    "sc_X_train = StandardScaler()\n",
    "X_train_scaled = sc_X_train.fit_transform(X_train)\n",
    "\n",
    "#create classifier\n",
    "# inn = INN.INN(**INN_parameters, \n",
    "#     n_blocks=hyperparameter_best_upper_confidence_bound_scaled[0], \n",
    "#     coupling_network_layers=[hyperparameter_best_upper_confidence_bound_scaled[2]] * hyperparameter_best_upper_confidence_bound_scaled[1]\n",
    "# )\n",
    "\n",
    "inn = INN.INN(**INN_parameters, \n",
    "    n_blocks=5, \n",
    "    coupling_network_layers=[512] * 2\n",
    ")\n",
    "\n",
    "X_train_scaled_cuda = torch.Tensor(X_train_scaled).to(device)\n",
    "y_train_cuda = torch.Tensor(y_train).to(device)\n",
    "\n",
    "# fit\n",
    "inn.fit(X_train_scaled_cuda, y_train_cuda, \n",
    "    n_epochs=16,\n",
    "    batch_size=batch_size//2,\n",
    "    optimizer=Adam(inn.parameters(), lr=lr), \n",
    "    loss_weights=loss_weights,\n",
    "    verbose=2,\n",
    ");\n",
    "\n",
    "del X_train_scaled_cuda, y_train_cuda"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 0: 100%|██████████| 4924/4924 [01:46<00:00, 46.12it/s, weighted_loss=-66.167, bce=+0.171, dvg=+9.225, rcst=+0.413, logdet=-77.514]\n",
      "Epoch 1: 100%|██████████| 4924/4924 [01:46<00:00, 46.36it/s, weighted_loss=-73.023, bce=+0.171, dvg=+10.206, rcst=+0.424, logdet=-85.367]\n",
      "Epoch 2: 100%|██████████| 4924/4924 [01:46<00:00, 46.26it/s, weighted_loss=-76.600, bce=+0.134, dvg=+9.909, rcst=+0.410, logdet=-88.263]\n",
      "Epoch 3: 100%|██████████| 4924/4924 [01:46<00:00, 46.17it/s, weighted_loss=-80.086, bce=+0.163, dvg=+8.080, rcst=+0.405, logdet=-90.199]\n",
      "Epoch 4: 100%|██████████| 4924/4924 [01:47<00:00, 46.01it/s, weighted_loss=-82.496, bce=+0.158, dvg=+7.837, rcst=+0.397, logdet=-92.311]\n",
      "Epoch 5: 100%|██████████| 4924/4924 [01:49<00:00, 45.01it/s, weighted_loss=-82.012, bce=+0.168, dvg=+8.476, rcst=+0.402, logdet=-92.569]\n",
      "Epoch 6: 100%|██████████| 4924/4924 [01:47<00:00, 45.72it/s, weighted_loss=-84.188, bce=+0.155, dvg=+7.681, rcst=+0.395, logdet=-93.811]\n",
      "Epoch 7: 100%|██████████| 4924/4924 [01:47<00:00, 45.96it/s, weighted_loss=-84.455, bce=+0.154, dvg=+8.064, rcst=+0.392, logdet=-94.452]\n",
      "Epoch 8: 100%|██████████| 4924/4924 [01:50<00:00, 44.66it/s, weighted_loss=-85.616, bce=+0.163, dvg=+7.394, rcst=+0.391, logdet=-95.031]\n",
      "Epoch 9: 100%|██████████| 4924/4924 [01:44<00:00, 46.94it/s, weighted_loss=-82.257, bce=+0.169, dvg=+10.651, rcst=+0.396, logdet=-94.990]\n",
      "Epoch 10: 100%|██████████| 4924/4924 [01:41<00:00, 48.42it/s, weighted_loss=-82.273, bce=+0.152, dvg=+11.062, rcst=+0.398, logdet=-95.250]\n",
      "Epoch 11: 100%|██████████| 4924/4924 [01:42<00:00, 47.91it/s, weighted_loss=-85.826, bce=+0.165, dvg=+7.761, rcst=+0.395, logdet=-95.633]\n",
      "Epoch 12: 100%|██████████| 4924/4924 [01:41<00:00, 48.60it/s, weighted_loss=-84.808, bce=+0.172, dvg=+8.300, rcst=+0.398, logdet=-95.224]\n",
      "Epoch 13: 100%|██████████| 4924/4924 [01:41<00:00, 48.58it/s, weighted_loss=-85.541, bce=+0.143, dvg=+7.844, rcst=+0.405, logdet=-95.222]\n",
      "Epoch 14: 100%|██████████| 4924/4924 [01:43<00:00, 47.69it/s, weighted_loss=-81.159, bce=+0.184, dvg=+10.784, rcst=+0.420, logdet=-94.202]\n",
      "Epoch 15: 100%|██████████| 4924/4924 [01:42<00:00, 48.26it/s, weighted_loss=-85.563, bce=+0.164, dvg=+8.282, rcst=+0.413, logdet=-95.897]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation on Test Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "with open('../data/data_test.pt', 'rb') as file:\n",
    "    X_test, y_test = pickle.load(file)\n",
    "\n",
    "print(f'{X_test.shape = }')\n",
    "print(f'{y_test.shape = }')\n",
    "\n",
    "X_test_scaled = torch.Tensor(sc_X_train.transform(X_test)).to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_test.shape = (630290, 28)\n",
      "y_test.shape = (630290, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "n_batches = len(X_test) // batch_size\n",
    "y_proba_pred = np.empty((0, 2))\n",
    "z_pred = np.empty((0, 26))\n",
    "for i_batch in tqdm(range(n_batches + 1)):\n",
    "    y_proba_pred_new, z_pred_new = inn.forward(X_test_scaled[i_batch * batch_size: (i_batch+1) * batch_size])\n",
    "    y_proba_pred = np.concatenate([y_proba_pred, y_proba_pred_new.detach().cpu().numpy()], axis=0)\n",
    "    z_pred = np.concatenate([z_pred, z_pred_new.detach().cpu().numpy()], axis=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 616/616 [00:09<00:00, 64.72it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "for i, y_label in enumerate(['hosp', 'death']):\n",
    "    print(f'--- {y_label} ---')\n",
    "    evaluation_results[y_label] = np.concatenate([1 - y_proba_pred[:, i].reshape(-1, 1), y_proba_pred[:, i].reshape(-1, 1)], axis=1)\n",
    "\n",
    "    print(f'binary cross-entropy: {np.round(log_loss(y_test[:, i], evaluation_results[y_label][:, 1]), 4)}')\n",
    "    print(f'brier loss: {brier_score_loss(y_test[:, i], evaluation_results[y_label][:, 1]).round(4)}')\n",
    "    print(f'accuracy: {accuracy_score(y_test[:, i], evaluation_results[y_label][:, 1].round()).round(4)}')\n",
    "    print('confusion matrix:')\n",
    "    print(confusion_matrix(y_test[:, i], (evaluation_results[y_label][:, 1] > 0.5).astype(int)))\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2688\n",
      "brier loss: 0.0625\n",
      "accuracy: 0.9282\n",
      "confusion matrix:\n",
      "[[582136   3486]\n",
      " [ 41767   2901]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0973\n",
      "brier loss: 0.0146\n",
      "accuracy: 0.9828\n",
      "confusion matrix:\n",
      "[[617708   3778]\n",
      " [  7083   1721]]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "with open('../evaluation_results/models/INN.pt', 'wb') as file:\n",
    "    pickle.dump(evaluation_results, file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}