{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import INN\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, brier_score_loss, accuracy_score, confusion_matrix\n",
    "\n",
    "import GPy\n",
    "import optunity as opt\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "retrain = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "with open('../../data/data_train.pt', 'rb') as file:\n",
    "    X_train, y_train = pickle.load(file)\n",
    "\n",
    "print(f'{X_train.shape = }')\n",
    "print(f'{y_train.shape = }')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train.shape = (2313665, 33)\n",
      "y_train.shape = (2313665, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "INN_parameters = {\n",
    "    'in_features': X_train.shape[1],\n",
    "    'out_features': y_train.shape[1],\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "loss_weights = {\n",
    "    'bce_factor': 10,\n",
    "    'dvg_factor': 1,\n",
    "    'logdet_factor': 1,\n",
    "    'rcst_factor': 1\n",
    "}\n",
    "\n",
    "lr = 5e-4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "hyperparameter_search_space_boundaries = {\n",
    "    'n_blocks': [1, 12],\n",
    "    'n_coupling_network_hidden_layers': [1, 5],\n",
    "    'n_coupling_network_hidden_nodes': [4, 512 + 256],\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "n_epochs = 32\n",
    "batch_size = 512"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def scale_hyperparameters(hyperparameters):\n",
    "    return np.array([h * (boundaries[1] - boundaries[0]) + boundaries[0] for h, boundaries in zip(hyperparameters, hyperparameter_search_space_boundaries.values())])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def GP_log_loss_upper_confidence_bound(n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes, gp):\n",
    "    mean, var = gp.predict_noiseless(np.array([[n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes]]))\n",
    "    return mean + np.sqrt(var)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load GP-Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(f'Loading Results ...')\n",
    "with open(f'../../hyperparameter_results/INN.pt', 'rb') as file:\n",
    "    Q, E = pickle.load(file)\n",
    "print(f'Loaded Results')\n",
    "\n",
    "GP = GPy.models.GPRegression(Q, E, kernel=GPy.kern.Matern52(3))\n",
    "GP.optimize(messages=False);"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Results ...\n",
      "Loaded Results\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find Best Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "hyperparameter_best_upper_confidence_bound = opt.minimize(\n",
    "    lambda **kwargs: GP_log_loss_upper_confidence_bound(gp=GP, **kwargs),\n",
    "    **{k: [0, 1] for k in hyperparameter_search_space_boundaries.keys()}\n",
    ")[0]\n",
    "\n",
    "hyperparameter_best_upper_confidence_bound_scaled = scale_hyperparameters(hyperparameter_best_upper_confidence_bound.values()).round().astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "best_sampled_hyperparameters = scale_hyperparameters(Q[np.argmin(E)]).round().astype(int)\n",
    "print(f'{best_sampled_hyperparameters=}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best_sampled_hyperparameters=array([  3,   1, 748])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "sc_X_train = StandardScaler()\n",
    "X_train_scaled = sc_X_train.fit_transform(X_train)\n",
    "\n",
    "if retrain:\n",
    "    for i in range(5):\n",
    "        # scale features\n",
    "\n",
    "        #create classifier\n",
    "        inn = INN.INN(**INN_parameters, \n",
    "            n_blocks=best_sampled_hyperparameters[0], \n",
    "            coupling_network_layers=[best_sampled_hyperparameters[2]] * best_sampled_hyperparameters[1]\n",
    "        )\n",
    "        inn.train()\n",
    "\n",
    "        X_train_scaled_cuda = torch.Tensor(X_train_scaled).to(device)\n",
    "        y_train_cuda = torch.Tensor(y_train).to(device)\n",
    "\n",
    "        # fit\n",
    "        loss_history = inn.fit(X_train_scaled_cuda, y_train_cuda, \n",
    "            n_epochs=n_epochs,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=Adam(inn.parameters(), lr=lr), \n",
    "            loss_weights=loss_weights,\n",
    "            verbose=1,\n",
    "        );\n",
    "\n",
    "        with open(f'../../evaluation_results/models/INN_{i}.pt', 'wb') as file:\n",
    "            pickle.dump(inn.to('cpu'), file)\n",
    "\n",
    "        with open(f'../../evaluation_results/loss_history/INN_{i}.pt', 'wb') as file:\n",
    "            pickle.dump(loss_history, file)\n",
    "\n",
    "        del inn, X_train_scaled_cuda, y_train_cuda\n",
    "\n",
    "else:\n",
    "    if os.path.exists('../../evaluation_results/models/INN.pt'):\n",
    "        with open('../../evaluation_results/models/INN.pt', 'rb') as file:\n",
    "            inn = pickle.load(file)\n",
    "    if os.path.exists('../../evaluation_results/loss_history/INN.pt'):\n",
    "        with open('../../evaluation_results/loss_history/INN.pt', 'rb') as file:\n",
    "            loss_history = pickle.load(file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "with open('../../data/data_test.pt', 'rb') as file:\n",
    "    X_test, y_test = pickle.load(file)\n",
    "\n",
    "print(f'{X_test.shape = }')\n",
    "print(f'{y_test.shape = }')\n",
    "\n",
    "X_test_scaled = torch.Tensor(sc_X_train.transform(X_test)).to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_test.shape = (578417, 33)\n",
      "y_test.shape = (578417, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "evaluation_results = {'hosp': [], 'death': []}\n",
    "\n",
    "for j in range(5):\n",
    "\n",
    "    with open(f'../../evaluation_results/models/INN_{j}.pt', 'rb') as file:\n",
    "        inn = pickle.load(file).to(device)\n",
    "\n",
    "    n_batches = len(X_test) // batch_size\n",
    "    y_proba_pred = np.empty((len(X_test), 2))\n",
    "    for i_batch in tqdm(range(n_batches + 1)):\n",
    "        y_proba_pred[i_batch * batch_size: (i_batch+1) * batch_size] = inn.forward(X_test_scaled[i_batch * batch_size: (i_batch+1) * batch_size])[0].detach().cpu().numpy()\n",
    "\n",
    "    for i, y_label in enumerate(['hosp', 'death']):\n",
    "        print(f'--- {y_label} ---')\n",
    "        evaluation_results[y_label].append(np.concatenate([1 - y_proba_pred[:, i].reshape(-1, 1), y_proba_pred[:, i].reshape(-1, 1)], axis=1))\n",
    "\n",
    "        print(f'binary cross-entropy: {np.round(log_loss(y_test[:, i], evaluation_results[y_label][-1][:, 1]), 4)}')\n",
    "        print(f'brier loss: {brier_score_loss(y_test[:, i], evaluation_results[y_label][-1][:, 1]).round(4)}')\n",
    "        print(f'accuracy: {accuracy_score(y_test[:, i], evaluation_results[y_label][-1][:, 1].round()).round(4)}')\n",
    "        print('confusion matrix:')\n",
    "        print(confusion_matrix(y_test[:, i], (evaluation_results[y_label][-1][:, 1] > 0.5).astype(int)))\n",
    "        print()\n",
    "        time.sleep(0.5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1130/1130 [00:01<00:00, 652.81it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2771\n",
      "brier loss: 0.0672\n",
      "accuracy: 0.919\n",
      "confusion matrix:\n",
      "[[525000  14022]\n",
      " [ 32808   6587]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0664\n",
      "brier loss: 0.0125\n",
      "accuracy: 0.9862\n",
      "confusion matrix:\n",
      "[[569011   1688]\n",
      " [  6300   1418]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1130/1130 [00:01<00:00, 622.04it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2172\n",
      "brier loss: 0.057\n",
      "accuracy: 0.9332\n",
      "confusion matrix:\n",
      "[[537200   1822]\n",
      " [ 36840   2555]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0624\n",
      "brier loss: 0.0129\n",
      "accuracy: 0.9849\n",
      "confusion matrix:\n",
      "[[567777   2922]\n",
      " [  5836   1882]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1130/1130 [00:01<00:00, 729.14it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2286\n",
      "brier loss: 0.0592\n",
      "accuracy: 0.931\n",
      "confusion matrix:\n",
      "[[535972   3050]\n",
      " [ 36838   2557]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0663\n",
      "brier loss: 0.0118\n",
      "accuracy: 0.9869\n",
      "confusion matrix:\n",
      "[[569591   1108]\n",
      " [  6448   1270]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1130/1130 [00:01<00:00, 739.14it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2789\n",
      "brier loss: 0.0667\n",
      "accuracy: 0.9202\n",
      "confusion matrix:\n",
      "[[526781  12241]\n",
      " [ 33941   5454]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0609\n",
      "brier loss: 0.0119\n",
      "accuracy: 0.987\n",
      "confusion matrix:\n",
      "[[569672   1027]\n",
      " [  6488   1230]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1130/1130 [00:01<00:00, 739.87it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2668\n",
      "brier loss: 0.0586\n",
      "accuracy: 0.9303\n",
      "confusion matrix:\n",
      "[[535295   3727]\n",
      " [ 36602   2793]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0663\n",
      "brier loss: 0.0132\n",
      "accuracy: 0.9847\n",
      "confusion matrix:\n",
      "[[567906   2793]\n",
      " [  6049   1669]]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "with open('../../evaluation_results/predictions/INN.pt', 'wb') as file:\n",
    "    pickle.dump(evaluation_results, file)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}