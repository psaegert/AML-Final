{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import pickle\r\n",
    "import time\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "from tqdm.auto import tqdm\r\n",
    "\r\n",
    "import INN\r\n",
    "import torch\r\n",
    "from torch.optim import Adam\r\n",
    "\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.metrics import log_loss, brier_score_loss, accuracy_score, confusion_matrix\r\n",
    "\r\n",
    "import GPy\r\n",
    "import optunity as opt\r\n",
    "\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "print(f'Using device: {device}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "retrain = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "with open('../../data/data_train.pt', 'rb') as file:\r\n",
    "    X_train, y_train = pickle.load(file)\r\n",
    "\r\n",
    "print(f'{X_train.shape = }')\r\n",
    "print(f'{y_train.shape = }')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train.shape = (2521156, 28)\n",
      "y_train.shape = (2521156, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "INN_parameters = {\r\n",
    "    'in_features': X_train.shape[1],\r\n",
    "    'out_features': y_train.shape[1],\r\n",
    "    'device': device\r\n",
    "}\r\n",
    "\r\n",
    "loss_weights = {\r\n",
    "    'bce_factor': 10,\r\n",
    "    'dvg_factor': 1,\r\n",
    "    'logdet_factor': 1,\r\n",
    "    'rcst_factor': 1\r\n",
    "}\r\n",
    "\r\n",
    "lr = 5e-4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "hyperparameter_search_space_boundaries = {\r\n",
    "    'n_blocks': [1, 12],\r\n",
    "    'n_coupling_network_hidden_layers': [1, 5],\r\n",
    "    'n_coupling_network_hidden_nodes': [4, 512 + 256],\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "n_epochs = 32\r\n",
    "batch_size = 512"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def scale_hyperparameters(hyperparameters):\r\n",
    "    return np.array([h * (boundaries[1] - boundaries[0]) + boundaries[0] for h, boundaries in zip(hyperparameters, hyperparameter_search_space_boundaries.values())])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def GP_log_loss_upper_confidence_bound(n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes, gp):\r\n",
    "    mean, var = gp.predict_noiseless(np.array([[n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes]]))\r\n",
    "    return mean + np.sqrt(var)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load GP-Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(f'Loading Results ...')\r\n",
    "with open(f'../../hyperparameter_results/INN.pt', 'rb') as file:\r\n",
    "    Q, E = pickle.load(file)\r\n",
    "print(f'Loaded Results')\r\n",
    "\r\n",
    "GP = GPy.models.GPRegression(Q, E, kernel=GPy.kern.Matern52(3))\r\n",
    "GP.optimize(messages=False);"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Results ...\n",
      "Loaded Results\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find Best Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "hyperparameter_best_upper_confidence_bound = opt.minimize(\r\n",
    "    lambda **kwargs: GP_log_loss_upper_confidence_bound(gp=GP, **kwargs),\r\n",
    "    **{k: [0, 1] for k in hyperparameter_search_space_boundaries.keys()}\r\n",
    ")[0]\r\n",
    "\r\n",
    "hyperparameter_best_upper_confidence_bound_scaled = scale_hyperparameters(hyperparameter_best_upper_confidence_bound.values()).round().astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "best_sampled_hyperparameters = scale_hyperparameters(Q[np.argmin(E)]).round().astype(int)\r\n",
    "print(f'{best_sampled_hyperparameters=}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best_sampled_hyperparameters=array([  3,   1, 748])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "if retrain:\r\n",
    "    for i in range(5):\r\n",
    "        # scale features\r\n",
    "        sc_X_train = StandardScaler()\r\n",
    "        X_train_scaled = sc_X_train.fit_transform(X_train)\r\n",
    "\r\n",
    "        #create classifier\r\n",
    "        inn = INN.INN(**INN_parameters, \r\n",
    "            n_blocks=best_sampled_hyperparameters[0], \r\n",
    "            coupling_network_layers=[best_sampled_hyperparameters[2]] * best_sampled_hyperparameters[1]\r\n",
    "        )\r\n",
    "        inn.train()\r\n",
    "\r\n",
    "        X_train_scaled_cuda = torch.Tensor(X_train_scaled).to(device)\r\n",
    "        y_train_cuda = torch.Tensor(y_train).to(device)\r\n",
    "\r\n",
    "        # fit\r\n",
    "        loss_history = inn.fit(X_train_scaled_cuda, y_train_cuda, \r\n",
    "            n_epochs=n_epochs,\r\n",
    "            batch_size=batch_size,\r\n",
    "            optimizer=Adam(inn.parameters(), lr=lr), \r\n",
    "            loss_weights=loss_weights,\r\n",
    "            verbose=1,\r\n",
    "        );\r\n",
    "\r\n",
    "        with open(f'../../evaluation_results/models/INN_{i}.pt', 'wb') as file:\r\n",
    "            pickle.dump(inn.to('cpu'), file)\r\n",
    "\r\n",
    "        with open(f'../../evaluation_results/loss_history/INN_{i}.pt', 'wb') as file:\r\n",
    "            pickle.dump(loss_history, file)\r\n",
    "\r\n",
    "        del inn, X_train_scaled_cuda, y_train_cuda\r\n",
    "\r\n",
    "else:\r\n",
    "    if os.path.exists('../../evaluation_results/models/INN.pt'):\r\n",
    "        with open('../../evaluation_results/models/INN.pt', 'rb') as file:\r\n",
    "            inn = pickle.load(file)\r\n",
    "    if os.path.exists('../../evaluation_results/loss_history/INN.pt'):\r\n",
    "        with open('../../evaluation_results/loss_history/INN.pt', 'rb') as file:\r\n",
    "            loss_history = pickle.load(file)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32/32 [26:03<00:00, 48.85s/it, batch=4860/4861, weighted_loss=-75.312, bce=+0.116, dvg=+8.347, rcst=+0.458, logdet=-85.273]\n",
      "100%|██████████| 32/32 [25:53<00:00, 48.53s/it, batch=4860/4861, weighted_loss=-74.539, bce=+0.178, dvg=+8.578, rcst=+0.463, logdet=-85.358]\n",
      "100%|██████████| 32/32 [24:52<00:00, 46.63s/it, batch=4860/4861, weighted_loss=-76.840, bce=+0.125, dvg=+7.933, rcst=+0.449, logdet=-86.474]\n",
      "100%|██████████| 32/32 [25:01<00:00, 46.91s/it, batch=4860/4861, weighted_loss=-74.860, bce=+0.138, dvg=+8.318, rcst=+0.455, logdet=-85.014]\n",
      "100%|██████████| 32/32 [25:11<00:00, 47.24s/it, batch=4860/4861, weighted_loss=-77.321, bce=+0.129, dvg=+7.636, rcst=+0.460, logdet=-86.710]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "with open('../../data/data_test.pt', 'rb') as file:\r\n",
    "    X_test, y_test = pickle.load(file)\r\n",
    "\r\n",
    "print(f'{X_test.shape = }')\r\n",
    "print(f'{y_test.shape = }')\r\n",
    "\r\n",
    "X_test_scaled = torch.Tensor(sc_X_train.transform(X_test)).to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_test.shape = (622230, 33)\n",
      "y_test.shape = (622230, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "evaluation_results = {'hosp': [], 'death': []}\r\n",
    "\r\n",
    "for j in range(5):\r\n",
    "\r\n",
    "    with open(f'../../evaluation_results/models/INN_{j}.pt', 'rb') as file:\r\n",
    "        inn = pickle.load(file).to(device)\r\n",
    "\r\n",
    "    n_batches = len(X_test) // batch_size\r\n",
    "    y_proba_pred = np.empty((len(X_test), 2))\r\n",
    "    for i_batch in tqdm(range(n_batches + 1)):\r\n",
    "        y_proba_pred[i_batch * batch_size: (i_batch+1) * batch_size] = inn.forward(X_test_scaled[i_batch * batch_size: (i_batch+1) * batch_size])[0].detach().cpu().numpy()\r\n",
    "\r\n",
    "    for i, y_label in enumerate(['hosp', 'death']):\r\n",
    "        print(f'--- {y_label} ---')\r\n",
    "        evaluation_results[y_label].append(np.concatenate([1 - y_proba_pred[:, i].reshape(-1, 1), y_proba_pred[:, i].reshape(-1, 1)], axis=1))\r\n",
    "\r\n",
    "        print(f'binary cross-entropy: {np.round(log_loss(y_test[:, i], evaluation_results[y_label][-1][:, 1]), 4)}')\r\n",
    "        print(f'brier loss: {brier_score_loss(y_test[:, i], evaluation_results[y_label][-1][:, 1]).round(4)}')\r\n",
    "        print(f'accuracy: {accuracy_score(y_test[:, i], evaluation_results[y_label][-1][:, 1].round()).round(4)}')\r\n",
    "        print('confusion matrix:')\r\n",
    "        print(confusion_matrix(y_test[:, i], (evaluation_results[y_label][-1][:, 1] > 0.5).astype(int)))\r\n",
    "        print()\r\n",
    "        time.sleep(0.5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1216/1216 [00:01<00:00, 724.91it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2284\n",
      "brier loss: 0.0598\n",
      "accuracy: 0.9295\n",
      "confusion matrix:\n",
      "[[575679   3968]\n",
      " [ 39900   2683]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0506\n",
      "brier loss: 0.012\n",
      "accuracy: 0.9867\n",
      "confusion matrix:\n",
      "[[612476   1502]\n",
      " [  6782   1470]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1216/1216 [00:01<00:00, 735.92it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2839\n",
      "brier loss: 0.0666\n",
      "accuracy: 0.9217\n",
      "confusion matrix:\n",
      "[[568667  10980]\n",
      " [ 37747   4836]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0876\n",
      "brier loss: 0.0136\n",
      "accuracy: 0.9848\n",
      "confusion matrix:\n",
      "[[611165   2813]\n",
      " [  6663   1589]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1216/1216 [00:01<00:00, 731.46it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2344\n",
      "brier loss: 0.0588\n",
      "accuracy: 0.9322\n",
      "confusion matrix:\n",
      "[[577727   1920]\n",
      " [ 40282   2301]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0544\n",
      "brier loss: 0.0119\n",
      "accuracy: 0.9868\n",
      "confusion matrix:\n",
      "[[612587   1391]\n",
      " [  6823   1429]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1216/1216 [00:01<00:00, 733.72it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2482\n",
      "brier loss: 0.0617\n",
      "accuracy: 0.927\n",
      "confusion matrix:\n",
      "[[572557   7090]\n",
      " [ 38316   4267]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.054\n",
      "brier loss: 0.011\n",
      "accuracy: 0.9882\n",
      "confusion matrix:\n",
      "[[613585    393]\n",
      " [  6949   1303]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1216/1216 [00:01<00:00, 734.23it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2558\n",
      "brier loss: 0.0629\n",
      "accuracy: 0.9257\n",
      "confusion matrix:\n",
      "[[570955   8692]\n",
      " [ 37547   5036]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0658\n",
      "brier loss: 0.0125\n",
      "accuracy: 0.9856\n",
      "confusion matrix:\n",
      "[[611607   2371]\n",
      " [  6596   1656]]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "with open('../../evaluation_results/predictions/INN.pt', 'wb') as file:\r\n",
    "    pickle.dump(evaluation_results, file)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "68764fea58fce3debd0ec160882a0d43aa3f505a880da5fb9e1c918b12b71a7c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}