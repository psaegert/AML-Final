{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import INN\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss, brier_score_loss, accuracy_score, confusion_matrix\n",
    "\n",
    "import GPy\n",
    "import optunity as opt\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "retrain = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "with open('../../data/data_train.pt', 'rb') as file:\n",
    "    X_train, y_train = pickle.load(file)\n",
    "\n",
    "print(f'{X_train.shape = }')\n",
    "print(f'{y_train.shape = }')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train.shape = (2313665, 33)\n",
      "y_train.shape = (2313665, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "INN_parameters = {\n",
    "    'in_features': X_train.shape[1],\n",
    "    'out_features': y_train.shape[1],\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "loss_weights = {\n",
    "    'bce_factor': 10,\n",
    "    'dvg_factor': 1,\n",
    "    'logdet_factor': 1,\n",
    "    'rcst_factor': 1\n",
    "}\n",
    "\n",
    "lr = 5e-4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "hyperparameter_search_space_boundaries = {\n",
    "    'n_blocks': [1, 12],\n",
    "    'n_coupling_network_hidden_layers': [1, 5],\n",
    "    'n_coupling_network_hidden_nodes': [4, 512 + 256],\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "n_epochs = 32\n",
    "batch_size = 512"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def scale_hyperparameters(hyperparameters):\n",
    "    return np.array([h * (boundaries[1] - boundaries[0]) + boundaries[0] for h, boundaries in zip(hyperparameters, hyperparameter_search_space_boundaries.values())])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def GP_log_loss_upper_confidence_bound(n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes, gp):\n",
    "    mean, var = gp.predict_noiseless(np.array([[n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes]]))\n",
    "    return mean + np.sqrt(var)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load GP-Results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "print(f'Loading Results ...')\n",
    "with open(f'../../hyperparameter_results/INN.pt', 'rb') as file:\n",
    "    Q, E = pickle.load(file)\n",
    "print(f'Loaded Results')\n",
    "\n",
    "GP = GPy.models.GPRegression(Q, E, kernel=GPy.kern.Matern52(3))\n",
    "GP.optimize(messages=False);"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Results ...\n",
      "Loaded Results\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Find Best Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "hyperparameter_best_upper_confidence_bound = opt.minimize(\n",
    "    lambda **kwargs: GP_log_loss_upper_confidence_bound(gp=GP, **kwargs),\n",
    "    **{k: [0, 1] for k in hyperparameter_search_space_boundaries.keys()}\n",
    ")[0]\n",
    "\n",
    "hyperparameter_best_upper_confidence_bound_scaled = scale_hyperparameters(hyperparameter_best_upper_confidence_bound.values()).round().astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "best_sampled_hyperparameters = scale_hyperparameters(Q[np.argmin(E)]).round().astype(int)\n",
    "print(f'{best_sampled_hyperparameters=}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best_sampled_hyperparameters=array([  3,   1, 748])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "if retrain:\n",
    "    for i in range(5):\n",
    "        # scale features\n",
    "        sc_X_train = StandardScaler()\n",
    "        X_train_scaled = sc_X_train.fit_transform(X_train)\n",
    "\n",
    "        #create classifier\n",
    "        inn = INN.INN(**INN_parameters, \n",
    "            n_blocks=best_sampled_hyperparameters[0], \n",
    "            coupling_network_layers=[best_sampled_hyperparameters[2]] * best_sampled_hyperparameters[1]\n",
    "        )\n",
    "        inn.train()\n",
    "\n",
    "        X_train_scaled_cuda = torch.Tensor(X_train_scaled).to(device)\n",
    "        y_train_cuda = torch.Tensor(y_train).to(device)\n",
    "\n",
    "        # fit\n",
    "        loss_history = inn.fit(X_train_scaled_cuda, y_train_cuda, \n",
    "            n_epochs=n_epochs,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=Adam(inn.parameters(), lr=lr), \n",
    "            loss_weights=loss_weights,\n",
    "            verbose=1,\n",
    "        );\n",
    "\n",
    "        with open(f'../../evaluation_results/models/INN_{i}.pt', 'wb') as file:\n",
    "            pickle.dump(inn.to('cpu'), file)\n",
    "\n",
    "        with open(f'../../evaluation_results/loss_history/INN_{i}.pt', 'wb') as file:\n",
    "            pickle.dump(loss_history, file)\n",
    "\n",
    "        del inn, X_train_scaled_cuda, y_train_cuda\n",
    "\n",
    "else:\n",
    "    if os.path.exists('../../evaluation_results/models/INN.pt'):\n",
    "        with open('../../evaluation_results/models/INN.pt', 'rb') as file:\n",
    "            inn = pickle.load(file)\n",
    "    if os.path.exists('../../evaluation_results/loss_history/INN.pt'):\n",
    "        with open('../../evaluation_results/loss_history/INN.pt', 'rb') as file:\n",
    "            loss_history = pickle.load(file)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 32/32 [29:03<00:00, 54.49s/it, batch=4517/4518, weighted_loss=-75.494, bce=+0.214, dvg=+7.326, rcst=+0.466, logdet=-85.426]\n",
      "100%|██████████| 32/32 [27:06<00:00, 50.83s/it, batch=4517/4518, weighted_loss=-73.286, bce=+0.163, dvg=+8.945, rcst=+0.467, logdet=-84.330]\n",
      " 81%|████████▏ | 26/32 [20:28<04:39, 46.60s/it, batch=2575/4518, weighted_loss=-72.812, bce=+0.204, dvg=+8.730, rcst=+0.469, logdet=-84.052]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('../../data/data_test.pt', 'rb') as file:\n",
    "    X_test, y_test = pickle.load(file)\n",
    "\n",
    "print(f'{X_test.shape = }')\n",
    "print(f'{y_test.shape = }')\n",
    "\n",
    "X_test_scaled = torch.Tensor(sc_X_train.transform(X_test)).to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_test.shape = (622230, 33)\n",
      "y_test.shape = (622230, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "evaluation_results = {'hosp': [], 'death': []}\n",
    "\n",
    "for j in range(5):\n",
    "\n",
    "    with open(f'../../evaluation_results/models/INN_{j}.pt', 'rb') as file:\n",
    "        inn = pickle.load(file).to(device)\n",
    "\n",
    "    n_batches = len(X_test) // batch_size\n",
    "    y_proba_pred = np.empty((len(X_test), 2))\n",
    "    for i_batch in tqdm(range(n_batches + 1)):\n",
    "        y_proba_pred[i_batch * batch_size: (i_batch+1) * batch_size] = inn.forward(X_test_scaled[i_batch * batch_size: (i_batch+1) * batch_size])[0].detach().cpu().numpy()\n",
    "\n",
    "    for i, y_label in enumerate(['hosp', 'death']):\n",
    "        print(f'--- {y_label} ---')\n",
    "        evaluation_results[y_label].append(np.concatenate([1 - y_proba_pred[:, i].reshape(-1, 1), y_proba_pred[:, i].reshape(-1, 1)], axis=1))\n",
    "\n",
    "        print(f'binary cross-entropy: {np.round(log_loss(y_test[:, i], evaluation_results[y_label][-1][:, 1]), 4)}')\n",
    "        print(f'brier loss: {brier_score_loss(y_test[:, i], evaluation_results[y_label][-1][:, 1]).round(4)}')\n",
    "        print(f'accuracy: {accuracy_score(y_test[:, i], evaluation_results[y_label][-1][:, 1].round()).round(4)}')\n",
    "        print('confusion matrix:')\n",
    "        print(confusion_matrix(y_test[:, i], (evaluation_results[y_label][-1][:, 1] > 0.5).astype(int)))\n",
    "        print()\n",
    "        time.sleep(0.5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1216/1216 [00:01<00:00, 724.91it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2284\n",
      "brier loss: 0.0598\n",
      "accuracy: 0.9295\n",
      "confusion matrix:\n",
      "[[575679   3968]\n",
      " [ 39900   2683]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0506\n",
      "brier loss: 0.012\n",
      "accuracy: 0.9867\n",
      "confusion matrix:\n",
      "[[612476   1502]\n",
      " [  6782   1470]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1216/1216 [00:01<00:00, 735.92it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2839\n",
      "brier loss: 0.0666\n",
      "accuracy: 0.9217\n",
      "confusion matrix:\n",
      "[[568667  10980]\n",
      " [ 37747   4836]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0876\n",
      "brier loss: 0.0136\n",
      "accuracy: 0.9848\n",
      "confusion matrix:\n",
      "[[611165   2813]\n",
      " [  6663   1589]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1216/1216 [00:01<00:00, 731.46it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2344\n",
      "brier loss: 0.0588\n",
      "accuracy: 0.9322\n",
      "confusion matrix:\n",
      "[[577727   1920]\n",
      " [ 40282   2301]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0544\n",
      "brier loss: 0.0119\n",
      "accuracy: 0.9868\n",
      "confusion matrix:\n",
      "[[612587   1391]\n",
      " [  6823   1429]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1216/1216 [00:01<00:00, 733.72it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2482\n",
      "brier loss: 0.0617\n",
      "accuracy: 0.927\n",
      "confusion matrix:\n",
      "[[572557   7090]\n",
      " [ 38316   4267]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.054\n",
      "brier loss: 0.011\n",
      "accuracy: 0.9882\n",
      "confusion matrix:\n",
      "[[613585    393]\n",
      " [  6949   1303]]\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1216/1216 [00:01<00:00, 734.23it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.2558\n",
      "brier loss: 0.0629\n",
      "accuracy: 0.9257\n",
      "confusion matrix:\n",
      "[[570955   8692]\n",
      " [ 37547   5036]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0658\n",
      "brier loss: 0.0125\n",
      "accuracy: 0.9856\n",
      "confusion matrix:\n",
      "[[611607   2371]\n",
      " [  6596   1656]]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('../../evaluation_results/predictions/INN.pt', 'wb') as file:\n",
    "    pickle.dump(evaluation_results, file)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}