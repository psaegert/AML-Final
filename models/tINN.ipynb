{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import INN\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss, brier_score_loss, accuracy_score, confusion_matrix\n",
    "\n",
    "import GPy\n",
    "import optunity as opt\n",
    "import sobol as sb\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "train = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "with open('../data/data_train.pt', 'rb') as file:\n",
    "    X_train, y_train = pickle.load(file)\n",
    "\n",
    "X_train = X_train[:300000]\n",
    "y_train = y_train[:300000]\n",
    "\n",
    "print(f'{X_train.shape = }')\n",
    "print(f'{y_train.shape = }')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train.shape = (300000, 28)\n",
      "y_train.shape = (300000, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameter Optimization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "INN_parameters = {\n",
    "    'in_features': X_train.shape[1],\n",
    "    'out_features': y_train.shape[1],\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "loss_weights = {\n",
    "    'bce_factor': 15,\n",
    "    'dvg_factor': 1,\n",
    "    'logdet_factor': .1,\n",
    "    'rcst_factor': .25\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "n_epochs = 3\n",
    "batch_size = 1024"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "hyperparameter_search_space_boundaries = {\n",
    "    'n_blocks': [1, 16],\n",
    "    'n_coupling_network_hidden_layers': [2, 5],\n",
    "    'n_coupling_network_hidden_nodes': [16, 1024],\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def scale_hyperparameters(hyperparameters):\n",
    "    return np.array([h * (boundaries[1] - boundaries[0]) + boundaries[0] for h, boundaries in zip(hyperparameters, hyperparameter_search_space_boundaries.values())])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross-Validation: Helper Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def count_parameters(model):\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        if not parameter.requires_grad: continue\n",
    "        param = parameter.numel()\n",
    "        total_params += param\n",
    "    return total_params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_mean_CV_Score(score_function, hyperparameters, progress_bar_kwargs=None):\n",
    "    n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes = hyperparameters\n",
    "\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=20210927)\n",
    "\n",
    "    log_loss_list = np.empty(5, dtype=np.float64)\n",
    "\n",
    "    for split_index, (fit_index, val_index) in enumerate(kf.split(X_train)):\n",
    "        # create splits\n",
    "        X_fit, X_val = X_train[fit_index], X_train[val_index]\n",
    "        y_fit, y_val = torch.Tensor(y_train[fit_index]).to(device), y_train[val_index]\n",
    "\n",
    "        # scale features\n",
    "        sc_X_fit = StandardScaler()\n",
    "        X_fit_scaled = torch.Tensor(sc_X_fit.fit_transform(X_fit)).to(device)\n",
    "        X_val_scaled = torch.Tensor(sc_X_fit.transform(X_val)).to(device)\n",
    "\n",
    "        # create classifier\n",
    "        inn = INN.INN(**INN_parameters, n_blocks=n_blocks, coupling_network_layers=[n_coupling_network_hidden_nodes] * n_coupling_network_hidden_layers).to(device)\n",
    "\n",
    "        inn.train()\n",
    "\n",
    "        # fit\n",
    "        inn.fit(\n",
    "            X_fit_scaled,\n",
    "            y_fit,\n",
    "            n_epochs=n_epochs,\n",
    "            batch_size=batch_size,\n",
    "            optimizer=Adam(inn.parameters(), lr=1e-4), \n",
    "            loss_weights=loss_weights,\n",
    "            verbose=1,\n",
    "            progress_bar_kwargs=progress_bar_kwargs\n",
    "        )\n",
    "\n",
    "        inn.eval()\n",
    "\n",
    "        del X_fit_scaled, y_fit\n",
    "\n",
    "        # evaluate\n",
    "        n_batches = len(X_val) // batch_size\n",
    "        y_proba_pred = np.empty((0, 2))\n",
    "        for i_batch in range(n_batches + 1):\n",
    "            y_proba_pred_new = inn.forward(X_val_scaled[i_batch * batch_size: (i_batch+1) * batch_size])[0].detach().cpu().numpy()\n",
    "            y_proba_pred = np.concatenate([y_proba_pred, y_proba_pred_new], axis=0)\n",
    "\n",
    "        log_loss_list[split_index] = score_function(y_val, y_proba_pred)\n",
    "\n",
    "        # count parameters\n",
    "        total_parameters = count_parameters(inn)\n",
    "\n",
    "        del inn, X_val_scaled\n",
    "\n",
    "    # penalize n_parameters and bce\n",
    "    return np.log(total_parameters) * np.mean(log_loss_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def expected_improvement(n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes, gp):\n",
    "    # compute E(q) and Var(q)\n",
    "    E_pred, Var_pred = gp.predict_noiseless(np.array([[n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes]]))\n",
    "\n",
    "    # compute gamma with the STD(q)\n",
    "    γ = (E_best - E_pred) / np.sqrt(Var_pred)\n",
    "\n",
    "    # return Expected Improvement\n",
    "    return (np.sqrt(Var_pred) * (γ * stats.norm.cdf(γ) + stats.norm.pdf(γ)))[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def initialize_GP(n_samples, progress=0):\n",
    "    Q_init = np.empty((n_samples, len(hyperparameter_search_space_boundaries)))\n",
    "    E_init = np.empty((n_samples, 1))\n",
    "\n",
    "    # initialize with sobol sequence between 0 and 1\n",
    "    for i in range(n_samples):\n",
    "        for j, boundaries in enumerate(hyperparameter_search_space_boundaries.values()):\n",
    "            Q_init[i, j] = sb.i4_sobol(len(hyperparameter_search_space_boundaries), i)[0][j]# * (boundaries[1] - boundaries[0]) + boundaries[0]\n",
    "\n",
    "    # compute scores for the initial hyperparameters\n",
    "    for i, hyperparameters in enumerate(Q_init):\n",
    "\n",
    "        # skip the ones that have already been computed\n",
    "        if progress > i:\n",
    "            continue\n",
    "\n",
    "        # scale hyperparameters according to their bounds and convert them to integers\n",
    "        hyperparameters_scaled = scale_hyperparameters(hyperparameters).round().astype(int)\n",
    "\n",
    "        # print the status\n",
    "        hyperparameters_dict = {key: hyperparameters_scaled[i] for i, key in enumerate(hyperparameter_search_space_boundaries.keys())}\n",
    "        print(f'{i+1}/{len(Q_init)}: {hyperparameters_dict}')\n",
    "        time.sleep(0.35)\n",
    "        \n",
    "        # compute cv score\n",
    "        E_init[i, :] = get_mean_CV_Score(log_loss, hyperparameters_scaled)\n",
    "        progress += 1\n",
    "\n",
    "        # save checkpoint\n",
    "        print('Storing Checkpoint...')\n",
    "        with open(f'../hyperparameter_results/INN.pt', 'wb') as file:\n",
    "            pickle.dump((Q_init, E_init), file)\n",
    "        with open(f'../hyperparameter_results/INN_progress.pt', 'wb') as file:\n",
    "            pickle.dump(progress, file)\n",
    "        print('Stored Checkpoint...')\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    return Q_init, E_init"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Optimization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "initial_n_samples = 4#16\n",
    "additional_n_samples = 12#64"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GP-Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "if train:\n",
    "    # load checkpoint if possible\n",
    "    if os.path.isfile('../hyperparameter_results/INN.pt') and os.path.isfile('../hyperparameter_results/INN_progress.pt'):\n",
    "        print('Loading Checkpoint...')\n",
    "        with open('../hyperparameter_results/INN.pt', 'rb') as file:\n",
    "            Q, E = pickle.load(file)\n",
    "        with open('../hyperparameter_results/INN_progress.pt', 'rb') as file:\n",
    "            progress = pickle.load(file)\n",
    "        print('Loaded Checkpoint')\n",
    "    else:\n",
    "        progress = 0\n",
    "    \n",
    "    # if not all initial hyperparameters have been tested, continue testing them\n",
    "    if progress < initial_n_samples:\n",
    "        print(f\"Initializing GP...\")\n",
    "        time.sleep(0.3)\n",
    "        Q, E = initialize_GP(initial_n_samples, progress=progress)\n",
    "        progress = initial_n_samples\n",
    "\n",
    "    # main GP training loop\n",
    "    print('Training GP...')\n",
    "    for k in range(progress - initial_n_samples, additional_n_samples):\n",
    "        # train Gaussian Process\n",
    "        GP = GPy.models.GPRegression(Q, E, kernel=GPy.kern.Matern52(3))\n",
    "        GP.optimize(messages=False)\n",
    "\n",
    "        # determine E_best (minimum value of E)\n",
    "        E_best = np.min(E)\n",
    "\n",
    "        # determine q_new (q with maximum expected improvement)\n",
    "        optimizer_output = opt.maximize(\n",
    "            lambda **kwargs: expected_improvement(gp=GP, **kwargs),\n",
    "            **{k: [0, 1] for k in hyperparameter_search_space_boundaries.keys()}\n",
    "        )[0]\n",
    "\n",
    "        # extract and scale new 'optimal' hyperparameters\n",
    "        q_new = np.array([optimizer_output[k] for k in hyperparameter_search_space_boundaries.keys()]).ravel()\n",
    "        q_new_scaled = scale_hyperparameters(q_new).round().astype(int)\n",
    "\n",
    "        # only for integer values: if the new hyperparameters have already been tested, the algorithm converged\n",
    "        for q in Q:\n",
    "            if (q_new == q).all():\n",
    "                print('GP Converged early.')\n",
    "                break\n",
    "\n",
    "        # print status\n",
    "        hyperparameters_dict = {key: q_new_scaled[i] for i, key in enumerate(hyperparameter_search_space_boundaries.keys())}\n",
    "        print(f'{k+1}/{additional_n_samples}: {hyperparameters_dict}')\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        # add q_new to the training set Q\n",
    "        Q = np.vstack((Q, q_new))\n",
    "\n",
    "        # add value to E\n",
    "        E = np.vstack((E, get_mean_CV_Score(log_loss, q_new_scaled).reshape(-1, 1)))\n",
    "\n",
    "        # save checkpoint\n",
    "        progress += 1\n",
    "        print('Storing Checkpoint...')\n",
    "        with open(f'../hyperparameter_results/INN.pt', 'wb') as file:\n",
    "            pickle.dump((Q, E), file)\n",
    "        with open(f'../hyperparameter_results/INN_progress.pt', 'wb') as file:\n",
    "            pickle.dump(progress, file)\n",
    "        print('Stored Checkpoint...')\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print('Completed Training')\n",
    "\n",
    "else:\n",
    "    print(f'Loading Results...')\n",
    "    with open(f'../hyperparameter_results/INN.pt', 'rb') as file:\n",
    "        Q, E = pickle.load(file)\n",
    "    print(f'Loaded Results')\n",
    "\n",
    "GP = GPy.models.GPRegression(Q, E, kernel=GPy.kern.Matern52(3))\n",
    "GP.optimize(messages=False);"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9/12: {'n_blocks': 13, 'n_coupling_network_hidden_layers': 5, 'n_coupling_network_hidden_nodes': 1006}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/3 [01:02<?, ?it/s, batch=141/234, weighted_loss=+4.766, bce=+0.140, dvg=+1.298, rcst=+0.323, logdet=+12.935]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-61c4ab8a6902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# add value to E\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_mean_CV_Score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_new_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# save checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7044d97632e6>\u001b[0m in \u001b[0;36mget_mean_CV_Score\u001b[0;34m(score_function, hyperparameters, progress_bar_kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         inn.fit(\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mX_fit_scaled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0my_fit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/UNI/Semester_6/AML/AML-Final/models/INN.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_epochs, batch_size, optimizer, loss_weights, verbose, progress_bar_kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GP-Prediction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# inverse_log_C_linspace = np.linspace(*np.array(hyperparameter_search_space_boundaries['inverse_log_C']) * np.array([1.1, 1.1]), 1000)\n",
    "\n",
    "# GP_predicted_log_loss_mean, GP_predicted_log_loss_std = {}, {}\n",
    "# for y_label in ['hosp', 'death']:\n",
    "#     GP_predicted_log_loss_mean[y_label], GP_predicted_log_loss_var = GP[y_label].predict_noiseless(inverse_log_C_linspace.reshape(-1, 1))\n",
    "#     GP_predicted_log_loss_std[y_label] = np.sqrt(GP_predicted_log_loss_var)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'inverse_log_C'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9613a48a958a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minverse_log_C_linspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameter_search_space_boundaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inverse_log_C'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mGP_predicted_log_loss_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGP_predicted_log_loss_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'hosp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'death'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mGP_predicted_log_loss_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGP_predicted_log_loss_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_noiseless\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse_log_C_linspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'inverse_log_C'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Find 'Best' Hyperparameter"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def GP_log_loss_upper_confidence_bound(n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes, gp):\n",
    "    mean, var = gp.predict_noiseless(np.array([[n_blocks, n_coupling_network_hidden_layers, n_coupling_network_hidden_nodes]]))\n",
    "    return mean + np.sqrt(var)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "hyperparameter_best_upper_confidence_bound = opt.minimize(\n",
    "    lambda **kwargs: GP_log_loss_upper_confidence_bound(gp=GP, **kwargs),\n",
    "    **{k: [0, 1] for k in hyperparameter_search_space_boundaries.keys()}\n",
    ")[0]\n",
    "\n",
    "hyperparameter_best_upper_confidence_bound_scaled = scale_hyperparameters(hyperparameter_best_upper_confidence_bound.values()).round().astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "hyperparameter_best_upper_confidence_bound_scaled"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([  2,   5, 199])"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Final Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# scale features\n",
    "sc_X_train = StandardScaler()\n",
    "X_train_scaled = sc_X_train.fit_transform(X_train)\n",
    "\n",
    "# create classifier\n",
    "inn = INN.INN(**INN_parameters, \n",
    "    n_blocks=hyperparameter_best_upper_confidence_bound_scaled[0], \n",
    "    coupling_network_layers=[hyperparameter_best_upper_confidence_bound_scaled[2]] * hyperparameter_best_upper_confidence_bound_scaled[1]\n",
    ")\n",
    "\n",
    "X_train_scaled_cuda = torch.Tensor(X_train_scaled).to(device)\n",
    "y_train_cuda = torch.Tensor(y_train).to(device)\n",
    "\n",
    "# fit\n",
    "inn.fit(X_train_scaled_cuda, y_train_cuda, \n",
    "    n_epochs=16,\n",
    "    batch_size=batch_size,\n",
    "    optimizer=Adam(inn.parameters(), lr=1e-4), \n",
    "    loss_weights=loss_weights,\n",
    "    verbose=2,\n",
    ");\n",
    "\n",
    "del X_train_scaled_cuda, y_train_cuda"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Epoch 0: 100%|██████████| 292/292 [00:04<00:00, 64.94it/s, weighted_loss=+7.221, bce=+0.173, dvg=+1.721, rcst=+0.427, logdet=+28.057]\n",
      "Epoch 1: 100%|██████████| 292/292 [00:04<00:00, 65.97it/s, weighted_loss=+6.420, bce=+0.150, dvg=+1.447, rcst=+0.284, logdet=+26.588]\n",
      "Epoch 2: 100%|██████████| 292/292 [00:04<00:00, 66.30it/s, weighted_loss=+5.801, bce=+0.144, dvg=+1.305, rcst=+0.227, logdet=+22.867]\n",
      "Epoch 3: 100%|██████████| 292/292 [00:04<00:00, 66.96it/s, weighted_loss=+5.355, bce=+0.147, dvg=+1.263, rcst=+0.229, logdet=+18.307]\n",
      "Epoch 4: 100%|██████████| 292/292 [00:04<00:00, 65.68it/s, weighted_loss=+5.501, bce=+0.158, dvg=+1.536, rcst=+0.252, logdet=+15.373]\n",
      "Epoch 5: 100%|██████████| 292/292 [00:04<00:00, 65.74it/s, weighted_loss=+5.391, bce=+0.146, dvg=+1.751, rcst=+0.238, logdet=+13.880]\n",
      "Epoch 6: 100%|██████████| 292/292 [00:04<00:00, 66.52it/s, weighted_loss=+4.762, bce=+0.143, dvg=+1.337, rcst=+0.199, logdet=+12.311]\n",
      "Epoch 7: 100%|██████████| 292/292 [00:04<00:00, 66.67it/s, weighted_loss=+4.630, bce=+0.146, dvg=+1.288, rcst=+0.215, logdet=+10.953]\n",
      "Epoch 8: 100%|██████████| 292/292 [00:04<00:00, 66.58it/s, weighted_loss=+4.553, bce=+0.140, dvg=+1.465, rcst=+0.222, logdet=+9.340]\n",
      "Epoch 9: 100%|██████████| 292/292 [00:04<00:00, 65.92it/s, weighted_loss=+4.286, bce=+0.144, dvg=+1.246, rcst=+0.225, logdet=+8.163]\n",
      "Epoch 10: 100%|██████████| 292/292 [00:04<00:00, 66.23it/s, weighted_loss=+4.385, bce=+0.148, dvg=+1.349, rcst=+0.210, logdet=+7.566]\n",
      "Epoch 11: 100%|██████████| 292/292 [00:04<00:00, 66.18it/s, weighted_loss=+4.106, bce=+0.143, dvg=+1.243, rcst=+0.217, logdet=+6.602]\n",
      "Epoch 12: 100%|██████████| 292/292 [00:04<00:00, 65.69it/s, weighted_loss=+3.987, bce=+0.138, dvg=+1.292, rcst=+0.218, logdet=+5.771]\n",
      "Epoch 13: 100%|██████████| 292/292 [00:04<00:00, 65.29it/s, weighted_loss=+4.025, bce=+0.147, dvg=+1.255, rcst=+0.229, logdet=+5.108]\n",
      "Epoch 14: 100%|██████████| 292/292 [00:04<00:00, 64.66it/s, weighted_loss=+3.824, bce=+0.137, dvg=+1.244, rcst=+0.227, logdet=+4.703]\n",
      "Epoch 15: 100%|██████████| 292/292 [00:04<00:00, 64.69it/s, weighted_loss=+3.797, bce=+0.138, dvg=+1.293, rcst=+0.228, logdet=+3.791]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation on Test Set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "with open('../data/data_test.pt', 'rb') as file:\n",
    "    X_test, y_test = pickle.load(file)\n",
    "\n",
    "print(f'{X_test.shape = }')\n",
    "print(f'{y_test.shape = }')\n",
    "\n",
    "X_test_scaled = torch.Tensor(sc_X_train.transform(X_test)).to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_test.shape = (630290, 28)\n",
      "y_test.shape = (630290, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "n_batches = len(X_test) // batch_size\n",
    "y_proba_pred = np.empty((0, 2))\n",
    "z_pred = np.empty((0, 26))\n",
    "for i_batch in tqdm(range(n_batches + 1)):\n",
    "    y_proba_pred_new, z_pred_new = inn.forward(X_test_scaled[i_batch * batch_size: (i_batch+1) * batch_size])\n",
    "    y_proba_pred = np.concatenate([y_proba_pred, y_proba_pred_new.detach().cpu().numpy()], axis=0)\n",
    "    z_pred = np.concatenate([z_pred, z_pred_new.detach().cpu().numpy()], axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "for i, y_label in enumerate(['hosp', 'death']):\n",
    "    print(f'--- {y_label} ---')\n",
    "    evaluation_results[y_label] = np.concatenate([1 - y_proba_pred[:, i].reshape(-1, 1), y_proba_pred[:, i].reshape(-1, 1)], axis=1)\n",
    "\n",
    "    print(f'binary cross-entropy: {np.round(log_loss(y_test[:, i], evaluation_results[y_label][:, 1]), 4)}')\n",
    "    print(f'brier loss: {brier_score_loss(y_test[:, i], evaluation_results[y_label][:, 1]).round(4)}')\n",
    "    print(f'accuracy: {accuracy_score(y_test[:, i], evaluation_results[y_label][:, 1].round()).round(4)}')\n",
    "    print('confusion matrix:')\n",
    "    print(confusion_matrix(y_test[:, i], (evaluation_results[y_label][:, 1] > 0.5).astype(int)))\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- hosp ---\n",
      "binary cross-entropy: 0.222\n",
      "brier loss: 0.0592\n",
      "accuracy: 0.9295\n",
      "confusion matrix:\n",
      "[[584609   1013]\n",
      " [ 43400   1268]]\n",
      "\n",
      "--- death ---\n",
      "binary cross-entropy: 0.0483\n",
      "brier loss: 0.012\n",
      "accuracy: 0.9869\n",
      "confusion matrix:\n",
      "[[621162    324]\n",
      " [  7913    891]]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "with open('../evaluation_results/models/tINN.pt', 'wb') as file:\n",
    "    pickle.dump(evaluation_results, file)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}